{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObn/3M9xrV6xM8uFAw99rx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil301290/Multiclass-BugPriority/blob/main/Multi_Target_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EFFICIENT MULTI-TARGET CLASSIFICATION FOR BUG PRIORITY AND RESOLUTION TIME PREDICTION"
      ],
      "metadata": {
        "id": "IZZGk7SP8R90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Getting preprocessed data"
      ],
      "metadata": {
        "id": "yYNELOBe1iE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBZutQzf1JBz"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/smadarab/multilabel-classification.git'\n",
        "!cp /content/multilabel-classification/preprocessed_data.csv /content/preprocessed_data.csv\n",
        "!rm -rf \"/content/multilabel-classification\"\n",
        "!pip install -q scikit-multilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calculated the Embeddings: [\"BERT\", \"ROBERTA\", \"GPT1\", \"GPT2\", \"GPT2-MEDIUM\", \"GPT2-LARGE\", \"GPT2-XL\"]"
      ],
      "metadata": {
        "id": "3XxCTuga1R43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 19YPN0XAPsJjIHHVWMOxwg7C2T6zb9Qfs -O Embeddings.zip\n",
        "!unzip -q Embeddings.zip -d Embeddings\n",
        "!rm -rf Embeddings.zip\n",
        "!ls \"/content/Embeddings\""
      ],
      "metadata": {
        "id": "iOb3Gd0w1NtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dimensions of generated Embeddings"
      ],
      "metadata": {
        "id": "7DkE1YkT1evH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_embeddings_shapes(embeddings_dir):\n",
        "    embedding_folders = [\"BERT\", \"ROBERTA\", \"GPT1\", \"GPT2\", \"GPT2-MEDIUM\", \"GPT2-LARGE\", \"GPT2-XL\"]\n",
        "\n",
        "    for folder in embedding_folders:\n",
        "        embedding_path = os.path.join(embeddings_dir, folder, 'embeddings.npy')\n",
        "        if os.path.exists(embedding_path):\n",
        "            embeddings = np.load(embedding_path)\n",
        "            print(f\"Shape of embeddings in {folder}: {embeddings.shape}\")\n",
        "        else:\n",
        "            print(f\"Embeddings file not found in {folder}\")\n",
        "\n",
        "embeddings_dir = \"/content/Embeddings\"\n",
        "print_embeddings_shapes(embeddings_dir)"
      ],
      "metadata": {
        "id": "1Ja-mTmc1cjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Import Libraries"
      ],
      "metadata": {
        "id": "Lekvw3qf1zev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc, confusion_matrix, classification_report, accuracy_score, matthews_corrcoef, f1_score, precision_score, recall_score\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
        "from sklearn.decomposition import PCA\n",
        "from joblib import Parallel, delayed\n",
        "import logging\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "firvLqIc10bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Load Data"
      ],
      "metadata": {
        "id": "EgDBtibd4410"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path):\n",
        "    try:\n",
        "        data = pd.read_csv(data_path, encoding='ISO-8859-15')\n",
        "        data.rename(columns={\"time\": \"Time\"}, inplace=True)\n",
        "        logging.info(\"Data loaded successfully.\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        logging.error(\"File not found. Please check the path and try again.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "TZscTIEs4-Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Data Preprocessing"
      ],
      "metadata": {
        "id": "C-nkzxo64-fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_combined_class(data):\n",
        "    data['Time_Priority'] = data['Time'].astype(str) + '_' + data['Priority'].astype(str)\n",
        "    return data\n",
        "\n",
        "def encode_combined_class(data):\n",
        "    le = LabelEncoder()\n",
        "    data['Time_Priority_Encoded'] = le.fit_transform(data['Time_Priority'])\n",
        "    return data, le"
      ],
      "metadata": {
        "id": "mVYJusRY5Fzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Correlation Calculation and Plotting"
      ],
      "metadata": {
        "id": "cGIRlvtB5JXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pearson_correlation(data):\n",
        "    correlation_matrix = data[['Time', 'Priority']].corr()\n",
        "    logging.info(f\"Pearson correlation matrix:\\n{correlation_matrix}\")\n",
        "    return correlation_matrix\n",
        "\n",
        "def plot_correlation_heatmap(correlation_matrix, filename_base):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title('Correlation Heatmap between Time and Priority')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{filename_base}.png\", dpi=600)\n",
        "    plt.savefig(f\"{filename_base}.pdf\", dpi=600)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2VD2qFt25Sef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Class Distribution Plotting"
      ],
      "metadata": {
        "id": "a4sElwOi5U1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution(data, column, title, filename_base):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(data=data, x=column, palette=\"viridis\")\n",
        "    plt.title(f\"Class Distribution {title}\")\n",
        "    plt.xlabel(f\"{column} Class\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    for p in plt.gca().patches:\n",
        "        height = p.get_height()\n",
        "        plt.gca().annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{filename_base}.png\", dpi=600)\n",
        "    plt.savefig(f\"{filename_base}.pdf\", dpi=600)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TRgotZWT5ZGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. SMOTE Function"
      ],
      "metadata": {
        "id": "v27fiTAn5fPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_smote(X, y):\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    logging.info(\"SMOTE applied.\")\n",
        "    logging.info(f\"X_resampled shape: {X_resampled.shape}\")\n",
        "    logging.info(f\"y_resampled shape: {y_resampled.shape}\")\n",
        "    return X_resampled, y_resampled"
      ],
      "metadata": {
        "id": "mFCHtvrm5byZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Split Combined Class"
      ],
      "metadata": {
        "id": "vNjVry3q59Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_combined_class(data, combined_col):\n",
        "    time_priority_split = data[combined_col].str.split('_', expand=True)\n",
        "    data['Time'] = time_priority_split[0].astype(int)\n",
        "    data['Priority'] = time_priority_split[1].astype(int)\n",
        "    logging.info(\"Combined class split into Time and Priority.\")\n",
        "    logging.info(data.head())\n",
        "    return data"
      ],
      "metadata": {
        "id": "dFWLWXVy6ALF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Load Embeddings"
      ],
      "metadata": {
        "id": "WQesvKs76NzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(embedding_dir):\n",
        "    return np.load(os.path.join(embedding_dir, 'embeddings.npy'))"
      ],
      "metadata": {
        "id": "bbeb6Yq06Pkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Dimensionality Reduction with PCA"
      ],
      "metadata": {
        "id": "Xc8Efjds6fKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_dimensionality_reduction(X_train_resampled, X_val_vect, X_test_vect, n_components):\n",
        "    results_dir = \"Results\"\n",
        "    reducer = PCA(n_components=n_components)\n",
        "    X_train_pca = reducer.fit_transform(X_train_resampled)\n",
        "    X_val_pca = reducer.transform(X_val_vect)\n",
        "    X_test_pca = reducer.transform(X_test_vect)\n",
        "\n",
        "    explained_variance = reducer.explained_variance_ratio_\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(np.cumsum(explained_variance))\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Variance Explained')\n",
        "    plt.title(f'Explained Variance by PCA Components (n_components={n_components})')\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, f'explained_variance_pca_{n_components}.png'), dpi=600)\n",
        "    plt.savefig(os.path.join(results_dir, f'explained_variance_pca_{n_components}.pdf'), dpi=600)\n",
        "    plt.show()\n",
        "\n",
        "    return X_train_pca, X_val_pca, X_test_pca, explained_variance"
      ],
      "metadata": {
        "id": "6xMVBU_t6iat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Cross Validation Hyperparameters"
      ],
      "metadata": {
        "id": "aojum6966pKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameter_tuning(model, param_grid, X, y, cv, search_type='grid'):\n",
        "    if search_type == 'grid':\n",
        "        search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "    elif search_type == 'random':\n",
        "        search = RandomizedSearchCV(model, param_grid, n_iter=50, cv=cv, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "\n",
        "    search.fit(X, y)\n",
        "    logging.info(f\"Best parameters for {model.__class__.__name__}: {search.best_params_}\")\n",
        "    logging.info(f\"Best cross-validation accuracy: {search.best_score_}\")\n",
        "    return search.best_estimator_"
      ],
      "metadata": {
        "id": "mwtlrfIC6kMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Model Evaluation"
      ],
      "metadata": {
        "id": "e1H89cEa7Kph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(transformed_model, X_test_pca, y_test, model_name, model_results_dir):\n",
        "    logging.info(f\"Evaluating model: {model_name}\")\n",
        "    predictions = transformed_model.predict(X_test_pca).toarray()\n",
        "    proba_predictions = transformed_model.predict_proba(X_test_pca).toarray() if hasattr(transformed_model, 'predict_proba') else None\n",
        "\n",
        "    if proba_predictions is not None:\n",
        "        np.save(os.path.join(model_results_dir, f'{model_name}_probabilities.npy'), proba_predictions)\n",
        "        logging.info(f\"Probabilities saved for {model_name}\")\n",
        "\n",
        "    y_test_combined = np.hstack((y_test['Time'].values.reshape(-1, 1), y_test['Priority'].values.reshape(-1, 1)))\n",
        "\n",
        "    results = {}\n",
        "    labels = ['Time', 'Priority']\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        y_true = y_test_combined[:, i]\n",
        "        y_pred = predictions[:, i]\n",
        "        report = save_classification_reports(y_true, y_pred, model_results_dir, label)\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='macro')\n",
        "        recall = recall_score(y_true, y_pred, average='macro')\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        results[label] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "            \"mcc\": mcc,\n",
        "            \"report\": report\n",
        "        }\n",
        "        if proba_predictions is not None:\n",
        "            y_scores = proba_predictions[:, i] if proba_predictions.ndim > 1 else proba_predictions\n",
        "            plot_metrics_aggregated(y_true, y_scores, y_pred, model_name, model_results_dir, label)\n",
        "\n",
        "        # Log the classification report\n",
        "        logging.info(f\"Classification Report for {model_name} - {label}:\\n{pd.DataFrame(report).transpose()}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "JwbN3K0K7MTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Plotting"
      ],
      "metadata": {
        "id": "7p5VEr_r7mOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics_aggregated(y_true, y_scores, y_pred, model_name, model_results_dir, label):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    n_classes = len(np.unique(y_true))\n",
        "    fpr, tpr, roc_auc, precision, recall, pr_auc = {}, {}, {}, {}, {}, {}\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        y_true_bin = (y_true == i).astype(int)\n",
        "        if y_scores.ndim == 1:\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_true_bin, y_scores)\n",
        "            precision[i], recall[i], _ = precision_recall_curve(y_true_bin, y_scores)\n",
        "        else:\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_true_bin, y_scores[:, 0])\n",
        "            precision[i], recall[i], _ = precision_recall_curve(y_true_bin, y_scores[:, 0])\n",
        "\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        pr_auc[i] = auc(recall[i], precision[i])\n",
        "\n",
        "        axs[0].plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
        "        axs[1].plot(recall[i], precision[i], lw=2, label=f'Class {i} (area = {pr_auc[i]:.2f})')\n",
        "\n",
        "    axs[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    axs[0].set_xlim([0.0, 1.0])\n",
        "    axs[0].set_ylim([0.0, 1.05])\n",
        "    axs[0].set_xlabel('False Positive Rate')\n",
        "    axs[0].set_ylabel('True Positive Rate')\n",
        "    axs[0].set_title('ROC', fontsize=14)\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "\n",
        "    axs[1].set_xlabel('Recall')\n",
        "    axs[1].set_ylabel('Precision')\n",
        "    axs[1].set_ylim([0.0, 1.05])\n",
        "    axs[1].set_xlim([0.0, 1.0])\n",
        "    axs[1].set_title('Precision-Recall Curve', fontsize=14)\n",
        "    axs[1].legend(loc=\"lower left\")\n",
        "\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    cax = axs[2].matshow(matrix, cmap=\"Blues\")\n",
        "    fig.colorbar(cax, ax=axs[2])\n",
        "    axs[2].set_xlabel('Predicted label')\n",
        "    axs[2].set_ylabel('True label')\n",
        "    axs[2].set_title(f'Confusion Matrix', fontsize=14)\n",
        "    for (i, j), val in np.ndenumerate(matrix):\n",
        "        axs[2].text(j, i, f'{val}', ha='center', va='center', color='black')\n",
        "\n",
        "    plt.subplots_adjust(top=0.85)\n",
        "    fig.suptitle(f'Performance Metrics for {model_name} - {label}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_results_dir, f'{model_name}_{label}_metrics_aggregated.png'), dpi=600)\n",
        "    plt.savefig(os.path.join(model_results_dir, f'{model_name}_{label}_metrics_aggregated.pdf'), dpi=600)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M0mQW3Tt7lTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Creating Results Directory and Saving Results"
      ],
      "metadata": {
        "id": "FXet2bWG7rB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_results_directory(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def save_classification_reports(y_true, y_pred, model_results_dir, label):\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
        "    report_df.to_csv(os.path.join(model_results_dir, f'classification_report_{label}.csv'))\n",
        "    return report\n",
        "\n",
        "def save_results_to_csv(results, filename):\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "al3QdHt07uoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Main Function:\n",
        "\n",
        "      - Loads and preprocesses the data.\n",
        "      - Splits the data into train, validation, and test sets.\n",
        "      - Applies SMOTE to the training set.\n",
        "      - Loads embeddings for each method and processes them.\n",
        "      - Applies PCA for dimensionality reduction.\n",
        "      - Performs cross-validation, hyperparameter tuning, model training, and evaluation.\n",
        "      - Saves the performance results to CSV files.\n",
        "      - Logs the completion of processing.\n"
      ],
      "metadata": {
        "id": "w4shp4cw7z4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    results_dir = \"Results\"\n",
        "    create_results_directory(results_dir)\n",
        "\n",
        "    data_path = \"/content/preprocessed_data.csv\"\n",
        "    data = load_data(data_path)\n",
        "\n",
        "    if data is not None:\n",
        "        logging.info(data.head())\n",
        "        data = data[['Priority', 'Summary', 'Time']].dropna()\n",
        "        logging.info(data.info())\n",
        "\n",
        "        # Create the combined Time_Priority class\n",
        "        data = create_combined_class(data)\n",
        "        logging.info(\"Combined class created.\")\n",
        "        logging.info(data.head())\n",
        "\n",
        "        # Encode the combined Time_Priority class\n",
        "        data, label_encoder = encode_combined_class(data)\n",
        "\n",
        "        # Calculate Pearson correlation between Time and Priority\n",
        "        correlation_matrix = calculate_pearson_correlation(data)\n",
        "\n",
        "        # Plot correlation heatmap\n",
        "        plot_correlation_heatmap(correlation_matrix, os.path.join(results_dir, 'correlation_heatmap_time_priority'))\n",
        "\n",
        "        # Plot the distribution of the combined Time_Priority class before SMOTE\n",
        "        plot_class_distribution(data, 'Time_Priority', \"Time_Priority Before SMOTE\", os.path.join(results_dir, 'class_distribution_time_priority_before_smote'))\n",
        "\n",
        "        # Prepare the dataset for SMOTE\n",
        "        y = data['Time_Priority_Encoded']\n",
        "\n",
        "        # Split the dataset into train, validation, and test sets (40:40:20)\n",
        "        train_val_idx, test_idx = train_test_split(data.index, test_size=0.2, random_state=42)\n",
        "        train_idx, val_idx = train_test_split(train_val_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "        X_train, X_val, X_test = data.loc[train_idx], data.loc[val_idx], data.loc[test_idx]\n",
        "        y_train, y_val, y_test = y.loc[train_idx], y.loc[val_idx], y.loc[test_idx]\n",
        "\n",
        "        logging.info(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}, X_test shape: {X_test.shape}\")\n",
        "        logging.info(f\"y_train shape: {y_train.shape}, y_val shape: {y_val.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "        vectorizing_methods = {\n",
        "            'bert': 'BERT',\n",
        "            'roberta': 'ROBERTA',\n",
        "            'gpt1': 'GPT1',\n",
        "            'gpt2': 'GPT2',\n",
        "            'gpt2-medium': 'GPT2-MEDIUM',\n",
        "            'gpt2-large': 'GPT2-LARGE',\n",
        "            'gpt2-xl': 'GPT2-XL'\n",
        "        }\n",
        "\n",
        "        models = {\n",
        "            \"SVC\": SVC(),\n",
        "            \"Random Forest\": RandomForestClassifier(),\n",
        "            \"Logistic Regression\": LogisticRegression(),\n",
        "            \"KNeighbors\": KNeighborsClassifier(),\n",
        "            \"Decision Tree\": DecisionTreeClassifier(),\n",
        "            \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "            \"AdaBoost\": AdaBoostClassifier(),\n",
        "            \"GaussianNB\": GaussianNB(),\n",
        "            \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "        }\n",
        "\n",
        "        param_grids = {\n",
        "            'SVC': {\n",
        "                'C': [0.1, 1, 10, 100],\n",
        "                'kernel': ['linear', 'rbf', 'poly'],\n",
        "                'gamma': ['scale', 'auto']\n",
        "            },\n",
        "            'Random Forest': {\n",
        "                'n_estimators': [10, 50, 100, 200],\n",
        "                'max_depth': [None, 10, 20, 30],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            },\n",
        "            'Logistic Regression': {\n",
        "                'C': [0.1, 1, 10, 100],\n",
        "                'max_iter': [100, 500, 1000]\n",
        "            },\n",
        "            'KNeighbors': {\n",
        "                'n_neighbors': [3, 5, 7, 9],\n",
        "                'weights': ['uniform', 'distance'],\n",
        "                'algorithm': ['ball_tree', 'kd_tree', 'brute']\n",
        "            },\n",
        "            'Decision Tree': {\n",
        "                'max_depth': [None, 10, 20, 30],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            },\n",
        "            'Gradient Boosting': {\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'max_depth': [3, 4, 5]\n",
        "            },\n",
        "            'AdaBoost': {\n",
        "                'learning_rate': [0.01, 0.1, 1],\n",
        "                'n_estimators': [50, 100, 200]\n",
        "            },\n",
        "            'GaussianNB': {\n",
        "                # GaussianNB does not have hyperparameters to tune.\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'max_depth': [3, 4, 5]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        transformations = {\n",
        "            \"Binary Relevance\": BinaryRelevance,\n",
        "            \"Classifier Chains\": ClassifierChain,\n",
        "            \"Label Powerset\": LabelPowerset\n",
        "        }\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        for method, embedding_dir in vectorizing_methods.items():\n",
        "            logging.info(f\"Using embeddings from {method}...\")\n",
        "            embeddings_dir = os.path.join(\"Embeddings\", embedding_dir)\n",
        "            embeddings = load_embeddings(embeddings_dir)\n",
        "\n",
        "            # Split the embeddings into train, validation, and test sets\n",
        "            X_train_vect = embeddings[train_idx]\n",
        "            X_val_vect = embeddings[val_idx]\n",
        "            X_test_vect = embeddings[test_idx]\n",
        "\n",
        "            # Apply SMOTE on the vectorized data\n",
        "            X_train_resampled, y_train_resampled = apply_smote(X_train_vect, y_train)\n",
        "\n",
        "            # Convert the resampled data back to a DataFrame for plotting\n",
        "            y_train_resampled_df = pd.DataFrame(y_train_resampled, columns=['Time_Priority_Encoded'])\n",
        "            y_train_resampled_df['Time_Priority'] = label_encoder.inverse_transform(y_train_resampled_df['Time_Priority_Encoded'])\n",
        "\n",
        "            # Plot the distribution of the combined Time_Priority class after SMote\n",
        "            method_results_dir = os.path.join(results_dir, method)\n",
        "            create_results_directory(method_results_dir)\n",
        "            plot_class_distribution(y_train_resampled_df, 'Time_Priority', f\"Time_Priority After SMOTE {method}\", os.path.join(method_results_dir, 'class_distribution_time_priority_after_smote'))\n",
        "\n",
        "            # Split Time and Priority back from Time_Priority for all sets\n",
        "            y_train_resampled_df = split_combined_class(y_train_resampled_df, 'Time_Priority')\n",
        "            y_val_df = split_combined_class(pd.DataFrame({'Time_Priority': label_encoder.inverse_transform(y_val)}, columns=['Time_Priority']), 'Time_Priority')\n",
        "            y_test_df = split_combined_class(pd.DataFrame({'Time_Priority': label_encoder.inverse_transform(y_test)}, columns=['Time_Priority']), 'Time_Priority')\n",
        "\n",
        "            # Plot the distribution of 'Time' and 'Priority' separately after SMOTE\n",
        "            plot_class_distribution(y_train_resampled_df, 'Time', f\"Time After SMOTE {method}\", os.path.join(method_results_dir, 'class_distribution_time_after_smote'))\n",
        "            plot_class_distribution(y_train_resampled_df, 'Priority', f\"Priority After SMOTE {method}\", os.path.join(method_results_dir, 'class_distribution_priority_after_smote'))\n",
        "\n",
        "            logging.info(\"Data after splitting combined class:\")\n",
        "            logging.info(y_train_resampled_df.head())\n",
        "            logging.info(y_val_df.head())\n",
        "            logging.info(y_test_df.head())\n",
        "\n",
        "            # Apply dimensionality reduction with PCA with 25 components\n",
        "            n_components = 25\n",
        "            X_train_pca, X_val_pca, X_test_pca, explained_variances = apply_dimensionality_reduction(X_train_resampled, X_val_vect, X_test_vect, n_components)\n",
        "\n",
        "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                if model_name in param_grids:\n",
        "                    logging.info(f\"Performing hyperparameter tuning for {model_name}...\")\n",
        "                    best_model = hyperparameter_tuning(model, param_grids[model_name], X_train_pca, y_train_resampled_df['Time_Priority_Encoded'], cv, search_type='grid')\n",
        "                else:\n",
        "                    best_model = model\n",
        "\n",
        "                for transform_name, transform in transformations.items():\n",
        "                    full_model_name = f\"{transform_name} with {model_name}\"\n",
        "                    model_specific_results_dir = os.path.join(method_results_dir, full_model_name.replace(' ', '_'))\n",
        "                    os.makedirs(model_specific_results_dir, exist_ok=True)\n",
        "                    logging.info(f\"Training {full_model_name} model...\")\n",
        "                    transformed_model = transform(best_model)\n",
        "                    transformed_model.fit(X_train_pca, np.hstack((y_train_resampled_df['Time'].values.reshape(-1, 1), y_train_resampled_df['Priority'].values.reshape(-1, 1))))\n",
        "                    model_results = evaluate_model(transformed_model, X_test_pca, y_test_df, full_model_name, model_specific_results_dir)\n",
        "                    all_results.append({\n",
        "                        \"Embedding Method\": method,\n",
        "                        \"Model\": full_model_name,\n",
        "                        \"Results\": model_results\n",
        "                    })\n",
        "                    logging.info(f\"{full_model_name} model trained.\")\n",
        "                    for label in model_results:\n",
        "                        logging.info(f\"{full_model_name} - {label} accuracy: {model_results[label]['accuracy']}\")\n",
        "                        logging.info(f\"Classification Report for {full_model_name} - {label}:\\n{pd.DataFrame(model_results[label]['report']).transpose()}\")\n",
        "\n",
        "        # Prepare results for CSV export\n",
        "        csv_results = []\n",
        "        for result in all_results:\n",
        "            embedding_method = result[\"Embedding Method\"]\n",
        "            model = result[\"Model\"]\n",
        "            for label, metrics in result[\"Results\"].items():\n",
        "                csv_results.append({\n",
        "                    \"Embedding Method\": embedding_method,\n",
        "                    \"Model\": model,\n",
        "                    \"Label\": label,\n",
        "                    \"Accuracy\": metrics[\"accuracy\"],\n",
        "                    \"Precision\": metrics[\"precision\"],\n",
        "                    \"Recall\": metrics[\"recall\"],\n",
        "                    \"F1 Score\": metrics[\"f1_score\"],\n",
        "                    \"MCC\": metrics[\"mcc\"]\n",
        "                })\n",
        "\n",
        "        # Save all results to a CSV file\n",
        "        results_csv_path = os.path.join(results_dir, \"model_performance_summary.csv\")\n",
        "        save_results_to_csv(csv_results, results_csv_path)\n",
        "\n",
        "        logging.info(\"Processing complete.\")\n",
        "    else:\n",
        "        logging.error(\"Data loading failed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "L7pD7a-21wM6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}